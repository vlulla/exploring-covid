---
title: "Exploring COVID-19 data"
subtitle: "(only for Indiana)"
auhtor: "[Vijay Lulla](https://vlulla.github.io)"
date: 2020.07.02
output:
  html_document:
    fig_caption: true
    toc: true
    number_sections: true
linkcolor: blue
geometry: margin=1in
bibliography: bibliography.bib
---

```{r global_options, echo=FALSE, include=FALSE}
options(nwarnings=5L)
knitr::opts_chunk$set(echo=TRUE, include=TRUE, collapse=FALSE, warning=FALSE, fig.width=8, fig.height=6, fig.path="figs/")
```

# Introduction {#intro}

I am very interested in visualizations. Especially, geospatial or geographic
visualization! It is all the more better if the visualization is interactive.

For my Data Incubator fellowship application challenge I tried to create
a spatial animation of daily _confirmed cases_ and _deaths_, in Indiana, from
COVID-19 as reported by Johns Hopkins University's interactive webiste
[@dong_interactive_2020].

## Motivation

One of my persistent _annoyances_ (and _frustrations_) is the central
incompatibility between how the commonly available spatial data is organized
and how the data needs to be organized to be used for analytics. The central
issue is this: _most spatial data is stored in **wide**[^wiki-link-wide-table]
format whereas most analytics software needs the data to be in **long**
format_. And, not to mention all the various minor details (esoteric file
formats of spatial data, missing/invalid data, R's automatic type conversion,
etc.) that need to be considered to successfully join spatial data with
attribute data before you can even do any analytics. Lest you think I have
been living under a rock I am quite aware, from my own past experiences and
also reading articles and blogs similar to this [NYT
article](https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html),
that data munging can take surprisingly more time than anticipated. But
despite my better judgement I thought: _how hard can this really be_?  Right
away you know that you are reading the words of a novice _data scientist_!


# Data and methods {#data-and-methods}

There are two data components needed to achieve my objective: 1) COVID-19 time
series data, and 2) Geographic boundaries data.

```{r, message=FALSE}
## load the libraries to get started...
library("data.table")
library("sf")
library("ggplot2")
```

## COVID-19 time series data

Johns Hopkins University (JHU) provides the data in CSV format at
<https://github.com/CSSEGISandData/COVID-19>. Exploring the folders,
I surmised that I needed to get the time series data. So, I downloaded the CSV
files `time_series_covid19_confirmed_US.csv` and
`time_series_covid19_deaths_US.csv`. Some of you might be wondering: _why the
heck didn't you just *clone* the repository?_ to which I reply: _sorry, I was
being naive_ [^git-ignorance].

Let's read the data and see what we have.

```{r, data}
confirmed <- fread("time_series_covid19_confirmed_US.csv", colClasses="character")
dim(confirmed)
confirmed[1:7, 1:15]
head(colnames(confirmed), 20)

deaths <- fread("time_series_covid19_deaths_US.csv", colClasses="character")
dim(deaths)
deaths[1:7, 1:15]
head(colnames(deaths), 20)
```

As evident, this data is organized in a wide format where rows correspond to
the counties of US and the columns correspond to attributes of the county and
a column for each date, starting 2020.01.22, when JHU started tracking data.
Looking at the dimensions, especially the number of columns, of these data
I _definitely_ needed to prepare the dataset in order to join it with the
spatial data.

The `FIPS` related formatting is because
`time_series_covid19_confirmed_US.csv` stores the `FIPS` column as a numeric
with a decimal 0 (see row 6)! In fact, I spent more than four hours trying to
chase data type issues[^r-automatic-data-conversion-issues] before finally
deciding to read the data with all columns as **character** vectors and then
do the necessary data conversion myself. I began by fixing `FIPS`, which is
the key column needed for joining the spatial data.

```{r, fips-fixup}
confirmed[, FIPS:=as.character(as.integer(FIPS))]
confirmed[nchar(FIPS)==4, FIPS:=sprintf("0%s",FIPS)]

deaths[, FIPS:=as.character(as.integer(FIPS))]
deaths[nchar(FIPS)==4, FIPS:=sprintf("0%s",FIPS)]
```

## Geographic boundary data

Anyone who has ever worked with any GIS software has inevitably had to deal
with [shapefiles](https://en.wikipedia.org/wiki/Shapefile). While they might
have been acceptable for a time period when they were created they are not
suited for modern times. The GIS community, at least the open source
geospatial community, realizes these shortcomings and is coming up with
alternative formats such as [GeoPackage](https://www.geopackage.org/),
[GeoJSON](https://geojson.org/). But visit any of the federal, state, or local
government websites, and you are very likely to run into spatial data provided
only as shapefile, usually offered in a zip archive. There are many issues
with shapefiles (just check out <http://switchfromshapefile.org/>)...but
I know how to deal with all of them...just convert it to a GeoPackage! So,
I downloaded the US county shapefile
(`cb_2018_us_county_500k.zip`)[^not-geodatabase] from US Census Bureau's
[Cartographic Boundary Files
- Shapefile](https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.html)
  and converted it to a geopackage.

```{r, gpkg}
## d <- st_read("cb_2018_us_county_500k.shp") 
## st_write(d, "counties.gpkg") 
## rm(d)
(counties_geo <- st_read("counties.gpkg", quiet=TRUE))
```

Some things to notice are:

1. The geospatial data has only 3233 counties whereas the timeseries data has
   3261 counties!  That is because it has some extra rows for `Grand
   Princess`, `Federal Corrections (MDOC)` etc. which are irrelevant for
   my purposes.
1. The `GEOID` column is comprised of concatenating `STATEFP` and `COUNTYFP`
   columns. You will also note that `table(nchar(counties_geo$GEOID))` yields
   `r table(nchar(counties_geo$GEOID))` and that's why we had to '0'-prefix
   some of our FIPS rows earlier.
1. Finally, it appears that the `GEOID` and `FIPS` columns  can be used to
   join/relate the attribute and spatial tables.

## Transforming data

While we can definitely use the tabular data with the geospatial data "as is",
there are a few things that we may want to consider:

1. Since this timeseries is updated daily, there will be more columns as newer
   data are added.
1. Filtering, especially date range based, is going to involve selecting
   different columns. For instance, exploring cases/deaths 
   between April 1, 2020 to June 15, 2020 will require figuring out which
   columns to select. And, then a little while later if we wish to explore
   cases/deaths between April 15, 2020 to June 30, 2020 then we will again
   have to figure out which columns to select.

   Contrast that with storing this data in a long format, where there are
   three columns: one for `date`, another for `confirmed cases`, and the third
   column for `deaths`. Now selecting based on date ranges is a simple matter
   of row filtering that can be done by R's builtin indexing or other data
   munging packages (`data.table`, `dplyr`, `tidyverse`, `reshape`).
1. Determining a date (or date ranges) based on some combination of values
   (either confirmed case or deaths or some transformation of them) are going
   to be rather tedious.

Hence, it would be good if we can convert this to long data format. By the
way, this is the same format in which many of the analytics oriented applications,
R included, view the data. That is, instead of thinking of a table consisting
of rows we should think of it as a collection of columns, each of which is
a vector of values! This is why inquiring `R` about a data.frame's length (for
instance `length(mtcars)`) will yield the number of columns (i.e.,
`dim(mtcars)[[2L]]` which is `r length(mtcars)`). This view of the data is
called an "inverted table" [@hui_apl_2020 pg 11] and is the predominant view
of columnar databases and analytic processing systems! Wide-to-long conversion
is commonly called a "melt" operation in R parlance. Therefore let us melt our data.

```{r, melting}
deaths.id.vars <- 1:12 ## head(colnames(deaths), 14) ... just to verify
confirmed.id.vars <- 1:11 ## head(colnames(confirmed), 13)

deaths.long <- melt(deaths, id.vars=colnames(deaths)[deaths.id.vars], 
					measure.vars=colnames(deaths)[-deaths.id.vars], 
					variable.name="date1", value.name="deaths")
confirmed.long <- melt(confirmed, id.vars=colnames(confirmed)[confirmed.id.vars], 
					   measure.vars=colnames(confirmed)[-confirmed.id.vars], 
					   variable.name="date1", value.name="confirmed_cases")

stopifnot(nrow(deaths) * (ncol(deaths)-length(deaths.id.vars)) == 
		  nrow(deaths.long))
stopifnot(nrow(confirmed) * (ncol(confirmed)-length(confirmed.id.vars)) == 
		  nrow(confirmed.long))

# just-in-case date formatting messes up we need the original date.
deaths.long[, `:=`(ddate1 = strftime(as.Date(as.character(date1), format="%m/%d/%y"), 
									 format="%Y-%m-%d")
				  ,deaths=as.integer(deaths) )]
confirmed.long[, `:=`(ddate1=strftime(as.Date(as.character(date1), format="%m/%d/%y"), 
									  format="%Y-%m-%d")
					 ,confirmed_cases = as.integer(confirmed_cases))]
```
We `melt` the data by using the first few columns as `id` variables and all
the date columns as `measure` variables. It is a good idea to use `stopifnot`
statements to ensure that the transform data conforms to what we are
expecting. Starting from `R` version 4.0 `stopifnot` now accepts custom error
messages via argument names to make argument checking easier. 

## Plotting the data

Now that we have our data, let us try to plot it.  While I wanted to create an animation for all the 
counties in US I ran into lots of issues. So, I decided to just do a small
example for Indiana counties.

```{r indiana_only, fig.show="hold", out.width="50%", fig.align="default"}
IN_geo <- counties_geo[grepl("^18", counties_geo$GEOID),]
deaths_IN <- deaths.long[grepl("^18", FIPS),]
confirmed_IN <- confirmed.long[grepl("^18", FIPS), ]

deaths_20200627 <- deaths_IN[ddate1==as.Date('2020-06-27'), ]
cases_20200627 <- confirmed_IN[ddate1==as.Date('2020-06-27'), ]

par(mfrow=c(1,2))
d <- merge(IN_geo, deaths_20200627, by.x="GEOID", by.y="FIPS")
plot(d[, "deaths"], main="Deaths 2020-06-27", breaks='kmeans')
d <- merge(IN_geo, cases_20200627, by.x="GEOID", by.y="FIPS")
plot(d[, "confirmed_cases"], main="Confirmed cases 2020-06-27", breaks='kmeans')
```

Now that we know how to create a plot let's try to create a whole bunch of
images which we can combine into an animation.

```{r, animation-related}
outdir <- "figs"
dts <- unique(deaths_IN$ddate1)

## for(d in dts[155:160]) {
for(d in dts) {
	deaths_png <- file.path(outdir, sprintf("deaths_%s.png", d))
	cases_png <- file.path(outdir, sprintf("cases_%s.png", d))

	dd <- deaths_IN[ddate1==d, .(FIPS, deaths, ddate1)]
	cc <- confirmed_IN[ddate1==d,.(FIPS, confirmed_cases, ddate1)]
	ii <- IN_geo[, c("NAME", "GEOID", "geom")]
	dat <- merge(ii, cc, by.x="GEOID", by.y="FIPS")
	dat$confirmed_cases <- dat$confirmed_cases/sum(dat$confirmed_cases)
	if(!file.exists(cases_png)) {
	  png(cases_png)
	  ## plot(dat[, "confirmed_cases"], main=d, key.pos=NULL, border=0L, breaks="kmeans")
	  plot(dat[, "confirmed_cases"], main=d, breaks="kmeans")
	  dev.off()
	}

	dat <- merge(ii, dd, by.x="GEOID", by.y="FIPS")
	dat$deaths <- dat$deaths/sum(dat$deaths)
	if(!file.exists(deaths_png)) {
	  png(deaths_png)
	  ## plot(dat["deaths"], main=d, key.pos=NULL, border=0L, breaks="kmeans")
	  plot(dat["deaths"], main=d, breaks="kmeans")
	  dev.off()
	}
}
```

And, finally we can combine all of these pngs into a gif using ImageMagick
using something like `convert figs/cases*.png cases.gif`. Here is the very
rudimentary result of my _unsuccessful_ attempt.

<center>
![Number of confirmed cases from
2020.01.22--2020.06.30](figs/00cases.gif)
</center>

# Lessons learned

1. Working with geospatial data is a lot trickier than anticipated. I tend to
   forget this fact quite often and have to make a conscious, and regular,
   effort to keep this in mind.
1. Use `git` and `make` more often so that I become familiar enough with these
   tools to use them _unconsciously_.
1. Try to always remeber:

   > It always takes longer than you expect, even if you take Hofstadter's
   > Law into account.                                -- Douglas Hofstadter

```{r sessioninfo}
sessionInfo()
```

[^wiki-link-wide-table]: <https://en.wikipedia.org/wiki/Wide_and_narrow_data>
[^git-ignorance]: Firstly, I rejoice that I can even comprehend what you mean
  by this question. Secondly, as far as I'm aware git is not an integral part of
  much of GIS (my own area of comfort) workflows. But, most important of all,
  I have not yet internalized, and integrated, git comprehensively into _my own_
  workflow!
[^r-automatic-data-conversion-issues]: I could never figure out whether it was
  R's automatic data conversion or my heavy customization that was causing all
  these issues. I tried running `R --vanilla` which only increased my
  problems...sigh!
[^not-geodatabase]: Since I was mostly interested in getting the data into
  R it did not matter whehter I chose shapefile or geodatabase. I would have
  **definitely chosen** geodatabase if the data size would have exceeded
  shapefile's limitations.

# References {#references}

